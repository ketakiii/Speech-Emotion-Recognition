# Speech-Emotion-Recognition

Emotions play a significant role in human mental life. It is a medium of expression of one’s perspective or one’s mental state to others. This is essential to our rational as well as intelligent decisions. Automatic Speech Emotion recognition has become a promising area of study in the field of human- computer interaction. Speech Emotion Recognition as the name suggests is identifying a person’s emotion using their speech. Recognizing a person’s emotion can be based on a number of factors like the words they use, facial expressions, body language, etc. With the advancement of human computer interaction and artificial intelligence, a machine can be trained to do all of the above tasks. In our project, we focus on the speech part of emotion detection. The motive behind making machines understand human emotions is to improve customer experience in every sector. Emotion being a very subjective term most datasets have been categorized into seven emotions namely anger, disgust, fear, happiness, sadness, surprise and neutral. Although there are so many emotions, the datasets have only these seven emotions since many of the emotions lie very close to one another and it is hard to distinguish between emotions many a times. Hence we try to represent emotions on 3 dimensions. The 3 dimensions across which these emotions are represented are valence, activation and dominance. Valence ranges from negative to positive, activation ranges from low to high and the dominance ranges from dominated to dominant. These on a graph are from -1 to +1. The emotions are then plotted on this graph based on the 3 factors. Neutral comes right in the middle of the graph, while anger and disgust lie close due to the similarity in their features. Joy and happiness lie close by in the same way. Apart from the acoustic features, the lexical features matter in identifying emotions, since understanding the context of a sentence is important in
interpreting emotions. 


Dataset:

RAVDESS - The Ryerson Audio-Visual Database of Emotional Speech and Music (RAVDESS) dataset is a multi-modal database of emotional speech and song that has been validated. RAVDESS has 1440 speech files and 1012 song files. This dataset contains recordings of 24 professional actors (12 females, 12 males) speaking in a neutral North American accent and vocalizing two lexically-matched phrases. There are calm, happy, sad, angry, afraid, surprise, and disgust expressions in speech, and there are calm, happy, sad, angry, and fearful emotions in song. On emotional validity, intensity, and genuineness, each file was assessed ten times. 247 people who were typical of untrained adult study volunteers from North America supplied ratings. A total of 72 people participated in the test-retest study : https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio

SAVEE - Surrey Audio-Visual Expressed Emotion (SAVEE) Four native English malespeakers (known as DC, JE, JK, KL), postgraduate students, and researchers at the University of Surrey, aged 27 to 31, contributed to the SAVEE database. Anger, contempt, fear, pleasure, sadness, and surprise are some of the psychologically distinct kinds of emotion : https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee

TESS - A set of 200 target words were spoken in the carrier phrase ”Say the word’ by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 stimuli in total. Two actresses were recruited from the Toronto area. Both actresses speak English as their first language, are university educated, and have musical training. Audiometric testing indicated thatboth actresses have thresholds within the normal range : https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess


